{
    "load_pre": true,
    "ne_label_size": 9,
    "re_label_size": -1,
    "qg_tag_size": 2,
    "pretrain_route": "../data/pretrained/roberta_large_model/",
    "pretrain_config_fn": "../data/checkpoint/QG_TF_NE_cmtl_fcn_no_pretrain/bert_config.json",
    "activation": "gelu",
    "dropout_prob": 0.2,
    "logit_dropout_prob": 0.2,
    "biaff_linear_dim": 256,
    "use_lstm": false,
    "checkpoint_route": "../data/checkpoint/QG_TF_NE_cmtl_fcn_no_pretrain/qg_tf_ne_dev.pth",
    "log_dir": "../result/QG_TF_Roberta/",
    "adam_beta2": 0.9,
    "bert_lr": 1e-5,
    "head_lr": 5e-5,
    "weight_decay": 1e-5,
    "layer_lr_decay_rate": 0.97,
    "total_epochs": 500,
    "warmup_rate": 0.01,
    "gama": 1,
    "alpha": 2.5,
    "beta": 2.5,
    "num_ne_epochs": 25,
    "start_with_ne": false,
    "train_batch": 64,
    "test_batch": 256,
    "gpus": "0",
    "remove_homo_vars": true,
    "is_causal_neqg": true,
    "original_ne": false,
    "private_encoder_arch": "fcn",
    "private_hid_dim": 1024,
    "label_emb_dim": 256,
    "gumble_tau": 0.05,
    "transfer_hidden": false,
    "transfer_label": true,
    "label_enc_warmup_epochs": 0
}